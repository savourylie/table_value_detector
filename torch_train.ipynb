{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from imageio import imread\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(Dataset):\n",
    "    \"\"\"The training table dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_path=None, y_path=None):\n",
    "        if x_path is None or y_path is None:\n",
    "            raise ValueError(\"No data source specified.\")\n",
    "        \n",
    "        x_filenames = glob(x_path + '*.png')\n",
    "        y_filenames = glob(y_path + '*.png')\n",
    "        \n",
    "        self.x_data = [torch.from_numpy(imread(filename).transpose(2, 0, 1)) for filename in x_filenames]\n",
    "        self.y_data = [torch.from_numpy(imread(filename).reshape(1, *imread(filename).shape)) for filename in y_filenames]\n",
    "        self.len = len(self.x_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TableDataset('/Users/calvinku/Dropbox/ThoroughAI/Side Projects/YangZhe/data/cell01/',\n",
    "                       '/Users/calvinku/Dropbox/ThoroughAI/Side Projects/YangZhe/data/xu_label_cell01/')\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 1413, 3) (332, 1413)\n",
      "(3, 332, 1413) (1, 332, 1413)\n"
     ]
    }
   ],
   "source": [
    "img = imread('/Users/calvinku/Dropbox/ThoroughAI/Side Projects/YangZhe/data/cell01/1.png')\n",
    "label = imread('/Users/calvinku/Dropbox/ThoroughAI/Side Projects/YangZhe/data/xu_label_cell01/1.png')\n",
    "\n",
    "print(img.shape, label.shape)\n",
    "print(img.transpose(2, 0, 1).shape, label.reshape(1, *label.shape).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.conv11 = nn.Conv2d(3, 64, 3, stride=1, padding=0)\n",
    "        self.conv12 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3, stride=1, padding=0)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3, stride=1, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv31 = nn.Conv2d(128, 512, 3, stride=1, padding=0)\n",
    "        self.conv32 = nn.Conv2d(512, 512, 3, stride=1, padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "                \n",
    "        self.deconv11 = nn.ConvTranspose2d(512, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.deconv12 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv21 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.deconv22 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv31 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=0)\n",
    "        self.deconv32 = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, padding=0)\n",
    "        self.unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.pool1(self.conv12(self.conv11(x))))\n",
    "        print(x.shape)\n",
    "#         x = self.relu2(self.pool2(self.conv22(self.conv21(x))))\n",
    "#         x = self.relu3(self.pool3(self.conv32(self.conv31(x))))\n",
    "        \n",
    "#         x = self.relu4(self.unpool1(self.deconv12(self.deconv11(x))))\n",
    "#         x = self.relu5(self.unpool2(self.deconv22(self.deconv21(x))))\n",
    "        x = self.unpool3(self.deconv32(self.deconv31(x)))\n",
    "        print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN32s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        print(x5.shape)\n",
    "        x = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        print(x.shape)\n",
    "        x = self.bn2(self.relu(self.deconv2(x)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        print(x.shape)\n",
    "        x = self.bn3(self.relu(self.deconv3(x)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        print(x.shape)\n",
    "        x = self.bn4(self.relu(self.deconv4(x)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        print(x.shape)\n",
    "        x = self.bn5(self.relu(self.deconv5(x)))  # size=(N, 32, x.H, x.W)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)                   # size=(N, n_class, x.H/1, x.W/1)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.vgg import VGG\n",
    "\n",
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "batch_size, n_class, h, w = 10, 20, 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 3, 522, 1385]), y shape: torch.Size([1, 1, 522, 1385])\n",
      "torch.Size([1, 64, 259, 690])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ec33216427d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x shape: {}, y shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-af1bce2b05d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         x = self.relu4(self.unpool1(self.deconv12(self.deconv11(x))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#         x = self.relu5(self.unpool2(self.deconv22(self.deconv21(x))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv31\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'indices'"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 3\n",
    "    \n",
    "#     # test output size\n",
    "#     vgg_model = VGGNet(requires_grad=True)\n",
    "#     input = torch.autograd.Variable(torch.randn(batch_size, 3, 224, 224))\n",
    "#     output = vgg_model(input)\n",
    "#     assert output['x5'].size() == torch.Size([batch_size, 512, 7, 7])\n",
    "\n",
    "#     fcn_model = FCN32s(pretrained_net=vgg_model, n_class=n_class)\n",
    "#     input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n",
    "#     output = fcn_model(input)\n",
    "#     assert output.size() == torch.Size([batch_size, n_class, h, w])\n",
    "\n",
    "#     print(\"Pass size check\")\n",
    "\n",
    "#     # test a random batch, loss should decrease\n",
    "\n",
    "    pretrained_net = VGGNet()\n",
    "#     fcn_model = FCN32s(pretrained_net=pretrained_net, n_class=1)\n",
    "    fcn_model = FCN()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(fcn_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            x, y = data\n",
    "            x, y = Variable(x.type(torch.FloatTensor), requires_grad=False), Variable(y.type(torch.FloatTensor), requires_grad=False)\n",
    "            \n",
    "            print(\"x shape: {}, y shape: {}\".format(x.shape, y.shape))\n",
    "            optimizer.zero_grad()\n",
    "            output = fcn_model(x)\n",
    "            output = nn.functional.sigmoid(output)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            print(\"iter{}, loss {}\".format(iter, loss.data[0]))\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
